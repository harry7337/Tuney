# -*- coding: utf-8 -*-
"""video-question-engine.ipynb


Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1MEAgxa6CDZTBi-bWGhPxQLpJbOj7bViX

# Necessary Libraries
Run the following cells
"""

"""# Enter YouTube Video Link Here"""

"""Enter Video ID Here:"""
url = "https://www.youtube.com/watch?v=-Prwp31Y9as&list=PLpLDwT9Mi15U6KZAswW5aClz6UJImXkPB&index=1&pp=iAQB"


import numpy as np
import pandas as pd
import re
import nltk
import urllib.parse
import psycopg2

from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

from pytube import YouTube
from collections import defaultdict
from bs4 import BeautifulSoup
from youtube_transcript_api import YouTubeTranscriptApi
from rank_bm25 import BM25Okapi


from flashtext import KeywordProcessor
kp = KeywordProcessor()

# from transformers import pipeline

# summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

"""# Read Topics and Sub-topics"""
def read_files():
  botany_df = pd.read_csv('data/botany_final.csv').drop(columns='Unnamed: 0')
  chemistry_df = pd.read_csv('data/chemistry_final.csv').drop(columns='Unnamed: 0')
  physics_df = pd.read_csv('data/physics_final.csv').drop(columns='Unnamed: 0')
  zoology_df = pd.read_csv('data/zoology_final.csv').drop(columns='Unnamed: 0')
  return botany_df,chemistry_df,physics_df,zoology_df

botany_df,chemistry_df,physics_df,zoology_df = read_files()

subject_codes = {'Botany':53,'Zoology':56,'Physics':55,'Chemistry':54}
code_to_subject = {v: k for k, v in subject_codes.items()}
code_to_subject_df = {53: botany_df,54: chemistry_df,55: physics_df,56: zoology_df}

"""# Add Keyword"""

kp.add_keyword('Botany',('Subject','Botany'))
kp.add_keyword('Zoology',('Subject','Zoology'))
kp.add_keyword('Physics',('Subject','Physics'))
kp.add_keyword('Chemistry',('Subject','Chemistry'))

for df in [botany_df,chemistry_df,physics_df,zoology_df]:
  for i,row in df.iterrows():
    subtopic,topic = row['Name'],row['Topic']
    kp.add_keyword(subtopic, ('Sub-Topic', subtopic))
    kp.add_keyword(topic, ('Topic', topic))

subject_identified_df = None
subject_identified = None

def get_video_id(url):
  url_data = urllib.parse.urlparse(url)
  query = urllib.parse.parse_qs(url_data.query)
  video_id = query['v'][0]
  return video_id

def generate_transcript(id):
  # transcript = YouTubeTranscriptApi.get_transcript(id, languages=['hi'])
  transcript_list = YouTubeTranscriptApi.list_transcripts(id)
  transcript = transcript_list.find_transcript(['hi'])
  # print(len(transcript))
  transcript = transcript.translate('en')
  # for text in transcript:
  #   t = text["text"]
		# if t != '[Music]': script += t + " "
  return transcript

def segment_transcript(transcript):
  window = 60*5 #seconds
  text = []
  temp = ''
  start = 0
  for segment in transcript.fetch():
    if segment['start'] - start >= window:
      text.append(temp)
      start = segment['start']
      temp = ''
    else:
      temp += segment['text'] + " "
  return text


"""# Keyword Extraction"""
def get_title_keywords(vid):
  title_keywords_found = kp.extract_keywords(vid.title)

  keywords = defaultdict(list)
  for tup in title_keywords_found:
    keywords[tup[0]].append(tup[1])
  return keywords

"""## Identify Subject from Video Title"""
def identify_subject(keywords):
  global subject_identified,subject_identified_df

  subject_matches = np.zeros((4,),dtype=int) #indices correspond to bio,chem,phy,zoo
  if "subject" in keywords:
    subject_identified = keywords["subject"]
    subject_identified = subject_codes[subject_identified]
  else: #find topics
    for topic in keywords['Topic']:
      for i,df in enumerate([botany_df,chemistry_df,physics_df,zoology_df]):
        if topic in df['Topic']:
          subject_matches[i]+=1
    for subtopic in keywords['Sub-Topic']:
      for i,df in enumerate([botany_df,chemistry_df,physics_df,zoology_df]):
        if subtopic in df['Name']:
          subject_matches[i]+=1
    
    if np.sum(subject_matches)==0:
      print("Couldn't identify subject from title")
      return
    subject_identified = 53+np.argmax(subject_matches)
  subject_identified_df = code_to_subject_df[subject_identified]
  print("Subject Detected is: ",code_to_subject[subject_identified])



"""## Get Topics and Sub-Topics from Transcript"""
def get_topic_subtopics(text):
  global subject_identified,subject_identified_df

  transcript_keywords = set()
  for segment in text:
    keywords_found = kp.extract_keywords(segment)
    for keyword in keywords_found:
      transcript_keywords.add(keyword)
  print(transcript_keywords)
  if subject_identified is None:
  
    subject_matches = np.zeros((4,),dtype=int) #indices correspond to bio,chem,phy,zoo
    for name,chapter in transcript_keywords:
      if name=="Subject":
        subject_identified = subject_codes[chapter]
        subject_matches[subject_identified-53]+=1
        break
      if name=="Topic":
        for i,df in enumerate([botany_df,chemistry_df,physics_df,zoology_df]):
          if chapter in df["Topic"]:
            subject_matches[i]+=1

      if name=="Sub-Topic":
        for i,df in enumerate([botany_df,chemistry_df,physics_df,zoology_df]):
          if chapter in df["Name"]:
            subject_matches[i]+=1
    subject_identified = 53+np.argmax(subject_matches)
    subject_identified_df = code_to_subject_df[subject_identified]
  print("Subject Detected is: ",code_to_subject[subject_identified])

  topic_codes = set()
  subtopics = [x for title,x in transcript_keywords if title=='Sub-Topic']
  for tup in transcript_keywords:
    if tup[0]=='Sub-Topic':
      subtopic=tup[1]
      subtopics.append(subtopic)
      try:
        code = subject_identified_df.loc[subject_identified_df['Name']==subtopic].iloc[0]['Topic-Code']
        topic_codes.add(code)
      except:
        continue
  return topic_codes,subtopics


"""
db_schema = ['id',
'question',
'options',
 'correctOptionIndex',
 'explanation',
 'createdAt',
 'updatedAt',
 'creatorId',
 'canvasQuestionId',
 'canvasQuizId',
 'deleted',
 'type',
 'paidAccess',
 'explanationMp4',
 'level',
 'jee',
 'sequenceId',
 'proofRead',
 'orignalQuestionId',
 'topicId',
 'subjectId',
 'lock_version',
 'ncert',
 'correctOptions',
 'sprinkler',
 'lockCount']
"""


"""## Connect to DB"""
def create_cursor(engine):
  cur = engine.cursor()
  cur.execute('SET search_path TO public;')
  return cur


# Query the database and obtain data as Python objects
def get_questions_from_db(cur,questions,topic_codes,subtopics):
  global subject_identified
  try:
    if len(topic_codes)==1:
      cur.execute('SELECT "id","question","topicId" FROM "Question" WHERE "subjectId"={subject_code} AND "topicId"={topic_codes};'.format(subject_code=subject_identified,topic_codes=list(topic_codes)[0]))
    else:
      cur.execute('SELECT "id","question","topicId" FROM "Question" WHERE "subjectId"={subject_code} AND "topicId" IN {topic_codes};'.format(subject_code=subject_identified,topic_codes=tuple(topic_codes)))
    row = cur.fetchone()
    while row!=None and row != (None,None,None):
      question = row[1]
      q_keywords = [keyword for title, keyword in kp.extract_keywords(question)]
      if len(set(q_keywords).intersection(set(subtopics)))>0: # if there are common keywords
        questions.append(question)
      row = cur.fetchone()
  except Exception as e:
    print(e)
    cur.close()
  return questions

"""# Ranking Questions"""
def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    soup = BeautifulSoup(text, "html.parser")
    text = soup.get_text()

    # Remove newline characters and extra whitespaces
    text = re.sub(r"[\n\r\t]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()

    # Remove punctuation and digits
    text = re.sub(r"[^a-zA-Z\s]", "", text)

    # Tokenization
    tokens = nltk.word_tokenize(text)

    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization (optional)
    # You may need to install the WordNet package for lemmatization:
    # nltk.download('wordnet')
    lemmatizer = nltk.WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    return tokens


def rank_questions(questions,subtopics):
  preprocessed_questions = [' '.join(preprocess_text(question)) for question in questions]

  # Tokenize the questions text
  tokenized_questions = [question.split() for question in preprocessed_questions]

  # preprocessed_keywords = preprocess_text(' '.join(subtopics))
  # Tokenize the keywords

  tokenized_keywords = set([keyword.lower() for keyword in subtopics])

  # Create a BM25 object
  bm25 = BM25Okapi(tokenized_questions)

  # Rank questions based on BM25 scores for each keyword
  ranked_questions = bm25.get_scores(tokenized_keywords)

  sorted_question_indices = np.argsort(ranked_questions)[::-1]
  top_questions = [questions[i] for i in sorted_question_indices]
  return top_questions

"""# Top Question"""

def get_questions(url):
  global subject_identified
  print("URL is: ",url)
  video_id = get_video_id(url)
  print("Video ID is: ",video_id)
  transcript = generate_transcript(video_id)
  text = segment_transcript(transcript)
  vid = YouTube(url)
  keywords = get_title_keywords(vid)
  print(keywords)
  identify_subject(keywords)
  topic_codes,subtopics = get_topic_subtopics(text)

  engine = psycopg2.connect(
      database="learner_development",
      user="learner",
      password="Deq05h0KiL6icSvS",
      host="neetprep-staging.cvvtorjqg7t7.ap-south-1.rds.amazonaws.com",
      port='5432'
  )
  cur = create_cursor(engine)
  questions = []
  questions = get_questions_from_db(cur,questions,topic_codes,subtopics)
  top_questions = rank_questions(questions,subtopics)
  subject_identified = None
  subject_identified_df = None
  return top_questions[:100]








